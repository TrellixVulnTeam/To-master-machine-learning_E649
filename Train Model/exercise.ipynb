{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. What Linear Regression training algorithm can you use if you have a training set with millions of features?**\n",
    "##### Gradient Descent: Vẫn đề là nếu có local minnimum thì khó thoát ra được. Nhưng vì hàm MSE(theta) là hàm lồi (Chọn bất kì 1 đoạn thẳng chúng sẽ không bao giờ cắt qua đường cong ==> không ccó local minimum. GD tốt cho mô hình hồi quy quyến tính\n",
    "(The problem is that if there is local minnimum, it is difficult to exit. But since MSE(theta) function is convex function (Choose any line segment they will never cross the curve ==> there is no local minimum GD is good for charming regression model)\n",
    "+ 1.1 Batch gradient descent: global minimum +- epsilon.Tùy thuộc vào épilon mà thuật toán nhanh hay chậm (Depending on epsilon and the algorithm is fast or slow)\n",
    "    >+ GD . batch problem: sử dụng toàn bộ tập huấn luyện để tính toán gradient at every steps (use the entire training set to compute the gradient at every steps) => slow if training set is large\n",
    "+ 1.2 stochastic gradient descent: \n",
    "    >+ Chỉ chọn một phiên bản ngẫu nhiêu trong tập huấn luyện ở mỗi bước và tính toán ggiá trị gradient dựa trên một phiên bản duy nhất đó. => Thuật toán trở nên nhanh hơn vì có ít thao tác với dữ liệu hơn ở mỗi lần lặp. (Pick only one random version of the training set at each step and compute the gradient value based on that single instance. => The algorithm becomes faster because there are fewer data manipulations at each iteration.)\n",
    "    >+ Kết quả thu được chỉ ở mức tối thiểu không bằng Batch gradient descent nhưng lại có thể tránh được local minimum nếu có.(The results obtained are only at a minimum not equal to Batch gradient descent, but the local minimum can be avoided if at all.)\n",
    "+ 1.3 Mini-batch gradient descent: \n",
    "    >+ Thay vì tính toán gradient descent trên tập huấn luyện đầy đủ (GD), hoặc trên 1 phiên bản (SGD), Mini_Batch GD tính toán gradient descent trên các tập hợp ngẫu nhiên gọi là các Mini_Batch (Instead of computing gradient descent on the full training set (GD), or on one instance (SGD), Mini_Batch GD computes gradient descent over random sets called Mini_Batchs.)\n",
    "    >+ Result is better SGD but but it's hard to pass the local minimum (nhưng khó vượt qua được local minimum).\n",
    "\n",
    ">+ **SGD và MBGD sẽ hoạt động tốt nhất vì cả hai đều không cần tải toàn bộ tập dữ liệu vào bộ nhớ để thực hiện 1 bước giảm độ dốc. Hàng loạt sẽ ổn với cảnh báo rằng bạn có đủ bộ nhớ để tải tất cả dữ liệu.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Suppose the features in your training set have very different scales: what algorithms might suffer from this, and how? What can you do about it?**\n",
    "+ Khi làm việc với tập dữ liệu. Chúng ta phải đối diện với vấn đề là giá trị giữa các đặc trưng ( feature) cách nhau quá xa. Điều này khiến cho việc chạy vòng lặp Gradient descent trở nên khó khăn hơn. Vì thế có một thủ thuật gọi là feature scaling giúp cho giá trị các feature gần nhau hơn và điều này khiến cho việc chạy Gradient descent được nhanh hơn\n",
    "+ Phương pháp **Normal Equation** không yêu cầu chuẩn hóa các tính năng, vì vậy nó vẫn không bị ảnh hưởng bởi các tính năng trong tập huấn luyện có tỷ lệ rất khác nhau.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?**\n",
    ">+ Ở hồi quy tuyến tính mô hình Gradient Descent là một hàm lồi. Cho nên không gặp vấn đề với các local minimum ( vì không có local minimum mà chỉ có duy nhât global minimum ở đáy thôi) - In linear regression, the Gradient Descent model is a convex function. So there's no problem with local minimums (because there's no local minimum, only a global minimum at the bottom)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?**\n",
    ">+ No. The issue is that stochastic gradient descent and mini-batch gradient descent have randomness built into them. This means that they can find their way to nearby the global optimum, but they generally don't converge. One way to help them converge is to gradually reduce the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?**\n",
    ">+ Nếu lỗi xác thực liên tục tăng lên sau mỗi kỷ nguyên, thì một khả năng là tỷ lệ học tập quá cao và thuật toán đang phân kỳ. Nếu lỗi đào tạo cũng tăng lên, thì đây rõ ràng là vấn đề và bạn nên giảm tỷ lệ học.\n",
    ">+ Tuy nhiên, nếu lỗi đào tạo không tăng lên, thì mô hình của bạn dang bi overfitting và bạn nên ngừng đào tạo và áp dụng các biện pháp khắc phục phổ biến cho việc ovefiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
    "+ Both Mini-batch and Stochastic gradient descent are not guarenteed to minimize the cost function after each step because the both have a degree of randomness built into them\n",
    "+ Mini-bath randomly chooses which training examples to perform gradient descent on while Stochastic randomly chooses a single example.\n",
    "+ A better option is to save the model at regular intervals. When the model has not improved for a long time you can revert to the saved models.\n",
    ">+ Cả hai bước giảm dần theo lô nhỏ và giảm độ dốc Stochastic đều không được đảm bảo để giảm thiểu hàm chi phí sau mỗi bước vì cả hai đều có mức độ ngẫu nhiên được tích hợp sẵn. Mini-bath chọn ngẫu nhiên các ví dụ đào tạo để thực hiện giảm độ dốc trong khi Stochastic chọn ngẫu nhiên một ví dụ duy nhất. Một lựa chọn tốt hơn là lưu mô hình theo các khoảng thời gian đều đặn. Khi mô hình không được cải thiện trong một thời gian dài, bạn có thể hoàn nguyên về các mô hình đã lưu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?**\n",
    "+ Stochastic gradient descent là nhanh nhất để hội tụ vì nó chỉ xem xét một ví dụ đào tạo tại một thời điểm.\n",
    "+ Batch gradient descent được đảm bảo hội tụ với tốc độ học đủ nhỏ nhưng sẽ mất nhiều thời gian\n",
    "+ Vì SGD và MBGD đều là ngẫu nhiên, một chiến lược bạn có thể sử dụng để giúp chúng hội tụ là giảm tốc độ học tập theo thời gian để nó thực hiện các bước giảm dần độ dốc ngày càng nhỏ khi nó đạt đến mức tối thiểu toàn cầu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Giả sử bạn đang sử dụng Polynomial Regression. Bạn vẽ các đường cong học tập và bạn nhận thấy rằng có một khoảng cách lớn giữa lỗi đào tạo và lỗi xác nhận. Điều gì đang xảy ra? Ba cách để giải quyết vấn đề này là gì?**\n",
    "\n",
    ">+ **Answer:** Khi bạn vẽ các đường cong học tập và nhận thấy có một khoảng cách lớn giữa lỗi đào tạo và lỗi xác thực, đó là đặc điểm của mô hình overfitting. \"Khoảng trống\" tồn tại đơn giản vì lỗi đào tạo thấp hơn lỗi xác nhận. Một cách để phát triển mô hình overfitting là cung cấp thêm dữ liệu đào tạo. Một chiến thuật khác là giảm độ phức tạp của mô hình. Bạn cũng có thể giảm số lượng tính năng trong dữ liệu của mình. Một điều cuối cùng cần thử là thêm chính quy hóa vào mô hình của bạn. L2 (Ridge Regression) hoặc L1 (Lasso) là những lựa chọn tốt.\n",
    ">+ Nếu mô hình hoạt động tốt trên tập dữ liệu huấn luyện nhưng lại kém trên tập dữ liệu xác thực =>> Overfitting (If the model performs well on the training data set but poorly on the validation dataset =>> Overfitting)\n",
    ">+ Nếu mô hình kém trên cả 2 => Underfitting\n",
    ">+ If model is underfitting training data, adding more trainig examples will not help. You need to use a more complex model or come up eith better features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter α or reduce it?**\n",
    "+ When the training error and validation error are close to each other and high that means your model is underfitting (i.e. it has high bias). You should try to reduce the regularization hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10. Why would you want to use: Ridge Regression instead of plain Linear Regression (i.e., without any regularization)? or Lasso instead of Ridge Regression or Elastic Net instead of Lasso?**\n",
    "+ Ridge Regression tốt nhưng nếu dữ liệu chỉ có một số tính năng hiệu quả thì nên chọn Lasso hoặc Elastic vì chúng có xu hướng giảm trọng số của các tính năng vô dụng. (Ridge Regression is good but if the data has only some effective features then Lasso or Elastic should be chosen as they tend to reduce the weight of useless features.)\n",
    "+ Ridge regression instead of Linear Regression?\n",
    "Use Ridge regression when your model is overfitting the training set. If you think only several features in your training set are useful, go with lasso or elastic net.\n",
    "+  Lasso thay vì hồi quy Ridge?\n",
    "Lasso thực hiện lựa chọn tính năng tự động bằng cách giảm trọng lượng của các tính năng quan trọng nhất. Cụ thể, hồi quy Lasso sử dụng hình phạt l1 có xu hướng đẩy trọng số xuống chính xác bằng không. Điều này dẫn đến các mô hình thưa thớt, trong đó tất cả các trọng số đều bằng 0 ngoại trừ các trọng số quan trọng nhất. Đây là một cách để thực hiện tự động lựa chọn tính năng, điều này rất tốt nếu bạn nghi ngờ rằng chỉ có một số tính năng thực sự quan trọng. Khi bạn không chắc chắn, bạn nên thích hồi quy Ridge hơn.\n",
    "+ Elastic Net thay vì Lasso? Elastic Net được ưu tiên hơn Lasso vì Lasso có thể hoạt động thất thường khi số lượng tính năng nhiều hơn số lượng trường hợp huấn luyện hoặc khi một số tính năng có tương quan chặt chẽ.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?**\n",
    "+ Use softmax Regression Because this is a multi-class classification problem - Not a multi-output problem\n",
    "+ Softmax regression does not handle multiple output classes (i.e. [indoor, daytime]). So you'll need to use two logistic regression classsifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12. Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'target',\n",
       " 'frame',\n",
       " 'target_names',\n",
       " 'DESCR',\n",
       " 'feature_names',\n",
       " 'filename',\n",
       " 'data_module']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[\"target\"] # It is multinomial classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 1.4, 0.2],\n",
       "       [1. , 1.4, 0.2],\n",
       "       [1. , 1.3, 0.2]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "X_with_bias[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tuanenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14f373a780c3e7357575556c76745c93ddecb4f48af6f5f128656f7f3c679194"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
